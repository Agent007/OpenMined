{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-29 20:37:35,375] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from syft.controller import tensors, models\n",
    "\n",
    "import syft\n",
    "import syft.interfaces.torch.actual_torch as actual_torch\n",
    "import syft.interfaces.torch as torch\n",
    "import syft.interfaces.torch.nn as nn\n",
    "import syft.interfaces.torch.nn.functional as F\n",
    "import syft.interfaces.torch.optim as optim\n",
    "from syft.interfaces.torch.autograd import Variable\n",
    "from syft.interfaces.torch.distributions import Categorical\n",
    "\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "gamma = 0.9\n",
    "seed = 543\n",
    "render = False\n",
    "log_interval = 100\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine = nn.Linear(4, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        action_scores = self.affine(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "    \n",
    "policy = Policy()\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(Variable(state))\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = select_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = syft.FloatTensor(torch.rand(10,2,2).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind = syft.IntTensor([1,1,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = out.index_select(ind,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[ 0.2941406   0.9989439 ]\n",
       "  [ 0.5207259   0.3590897 ]]\n",
       "\n",
       " [[ 0.2941406   0.9989439 ]\n",
       "  [ 0.5207259   0.3590897 ]]\n",
       "\n",
       " [[ 0.1953882   0.9048445 ]\n",
       "  [ 0.226079    0.5687689 ]]\n",
       "\n",
       " [[ 0.3615693   0.5218327 ]\n",
       "  [ 0.1339474   0.6470152 ]]\n",
       "\n",
       " [[ 0.575147    0.09562236]\n",
       "  [ 0.8301317   0.8498407 ]]]\n",
       "[syft.FloatTensor:4 grad:None size:5x2x2 c:[] p:[3] init:view_5_2_2]\n",
       "\n",
       "\t-----------creators-----------\n",
       "\t[syft.FloatTensor:3 grad:None size:1x5x4 c:[4] p:[1] init:index_select_dim:0_int-id:1]\n",
       "\t------------------------------\n",
       "\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[ 0.3051162   0.9820263 ]\n",
       "  [ 0.1147337   0.7638507 ]]\n",
       "\n",
       " [[-0.2941406  -0.9989439 ]\n",
       "  [-0.5207259  -0.3590897 ]]\n",
       "\n",
       " [[ 0.3861234   0.8082908 ]\n",
       "  [ 0.2876182   0.5556896 ]]\n",
       "\n",
       " [[ 0.          0.        ]\n",
       "  [ 0.          0.        ]]\n",
       "\n",
       " [[ 0.          0.        ]\n",
       "  [ 0.          0.        ]]\n",
       "\n",
       " [[ 0.          0.        ]\n",
       "  [ 0.          0.        ]]\n",
       "\n",
       " [[ 0.7655473   0.07898402]\n",
       "  [ 0.857523    0.01522237]]\n",
       "\n",
       " [[ 0.2545201   0.9170496 ]\n",
       "  [ 0.3031685   0.1795458 ]]\n",
       "\n",
       " [[ 0.1431879   0.3000262 ]\n",
       "  [ 0.5735351   0.4295186 ]]\n",
       "\n",
       " [[ 0.4836702   0.3399992 ]\n",
       "  [ 0.1947275   0.1342559 ]]]\n",
       "[syft.FloatTensor:8 grad:None size:10x2x2 c:[] p:[7] init:view_10_2_2]\n",
       "\n",
       "\t-----------creators-----------\n",
       "\t[syft.FloatTensor:7 grad:None size:1x10x4 c:[8] p:[1] init:index_add_dim:0_1_5]\n",
       "\t------------------------------\n",
       "\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.index_add(ind,0,subset.neg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-25 14:32:57,551] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tLast length:    98\tAverage length: 66.12\n",
      "Episode 200\tLast length:    17\tAverage length: 85.61\n",
      "Episode 300\tLast length:    49\tAverage length: 80.06\n",
      "Episode 400\tLast length:   169\tAverage length: 138.29\n",
      "Episode 500\tLast length:    41\tAverage length: 135.51\n",
      "Episode 600\tLast length:    88\tAverage length: 126.97\n",
      "Episode 700\tLast length:   177\tAverage length: 163.88\n",
      "Episode 800\tLast length:   126\tAverage length: 179.32\n",
      "Episode 900\tLast length:    63\tAverage length: 138.71\n",
      "Episode 1000\tLast length:   199\tAverage length: 139.44\n",
      "Episode 1100\tLast length:   199\tAverage length: 176.30\n",
      "Episode 1200\tLast length:   199\tAverage length: 189.80\n",
      "Solved! Running reward is now 195.00467036348365 and the last episode runs to 199 time steps!\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(policy.parameters(), lr=0.15)\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "\n",
    "\n",
    "running_reward = 10\n",
    "for i_episode in count(1):\n",
    "    state = env.reset()\n",
    "    for t in range(10000):  # Don't infinite loop while learning\n",
    "        action = select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if render:\n",
    "            env.render()\n",
    "        policy.rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    running_reward = running_reward * 0.99 + t * 0.01\n",
    "    finish_episode()\n",
    "    if i_episode % log_interval == 0:\n",
    "        print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "            i_episode, t, running_reward))\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
