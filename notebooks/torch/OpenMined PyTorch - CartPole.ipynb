{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-30 02:35:04,405] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from syft.controller import tensors, models\n",
    "\n",
    "\n",
    "import syft\n",
    "import syft.interfaces.torch.actual_torch as actual_torch\n",
    "import syft.interfaces.torch as torch\n",
    "import syft.interfaces.torch.nn as nn\n",
    "import syft.interfaces.torch.nn.functional as F\n",
    "import syft.interfaces.torch.optim as optim\n",
    "from syft.interfaces.torch.autograd import Variable\n",
    "from syft.interfaces.torch.distributions import Categorical\n",
    "\n",
    "# import torch as torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.autograd import Variable\n",
    "# from torch.distributions import Categorical\n",
    "\n",
    "gamma = 0.9\n",
    "seed = 543\n",
    "render = False\n",
    "log_interval = 100\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine = nn.Linear(4, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        action_scores = self.affine(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "    \n",
    "policy = Policy()\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(Variable(state))\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = syft.FloatTensor([0.1,0.2,0.3,0.4,0.5],autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.log().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ 10.         5.         3.333333   2.5        2.      ]\n",
       "[syft.FloatTensor:14 grad:None size:5 c:[] p:[10, 12] init:mul_elem]\n",
       "\n",
       "\t-----------creators-----------\n",
       "\t[syft.FloatTensor:10 grad:None size:5 c:[14] p:[] init:emptyTensorCopy]\n",
       "\t[syft.FloatTensor:12 grad:None size:5 c:[14] p:[11, 13] init:pow_scalar]\n",
       "\t------------------------------\n",
       "\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.actual_torch.torch.autograd.Variable(torch.actual_torch.torch.FloatTensor(x.to_numpy()),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt2 = xt.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt2.backward(xt/xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 10.0000\n",
       "  5.0000\n",
       "  3.3333\n",
       "  2.5000\n",
       "  2.0000\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for t in range(10000):  # Don't infinite loop while learning\n",
    "    action = select_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if render:\n",
    "        env.render()\n",
    "    policy.rewards.append(reward)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-25 14:32:57,551] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tLast length:    98\tAverage length: 66.12\n",
      "Episode 200\tLast length:    17\tAverage length: 85.61\n",
      "Episode 300\tLast length:    49\tAverage length: 80.06\n",
      "Episode 400\tLast length:   169\tAverage length: 138.29\n",
      "Episode 500\tLast length:    41\tAverage length: 135.51\n",
      "Episode 600\tLast length:    88\tAverage length: 126.97\n",
      "Episode 700\tLast length:   177\tAverage length: 163.88\n",
      "Episode 800\tLast length:   126\tAverage length: 179.32\n",
      "Episode 900\tLast length:    63\tAverage length: 138.71\n",
      "Episode 1000\tLast length:   199\tAverage length: 139.44\n",
      "Episode 1100\tLast length:   199\tAverage length: 176.30\n",
      "Episode 1200\tLast length:   199\tAverage length: 189.80\n",
      "Solved! Running reward is now 195.00467036348365 and the last episode runs to 199 time steps!\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(policy.parameters(), lr=0.15)\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "\n",
    "\n",
    "running_reward = 10\n",
    "for i_episode in count(1):\n",
    "    state = env.reset()\n",
    "    for t in range(10000):  # Don't infinite loop while learning\n",
    "        action = select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if render:\n",
    "            env.render()\n",
    "        policy.rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    running_reward = running_reward * 0.99 + t * 0.01\n",
    "    finish_episode()\n",
    "    if i_episode % log_interval == 0:\n",
    "        print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "            i_episode, t, running_reward))\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
